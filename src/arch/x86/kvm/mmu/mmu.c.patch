--- /home/wangzc/Desktop/experiment/linux-source-5.13.0/arch/x86/kvm/mmu/mmu.c	2022-03-24 23:34:38.000000000 +0800
+++ /home/wangzc/Desktop/experiment/linux-source-5.13.0-kvm-shadow-ept/arch/x86/kvm/mmu/mmu.c	2022-04-19 21:54:39.999984136 +0800
@@ -1552,6 +1552,7 @@
 	hlist_del(&sp->hash_link);
 	list_del(&sp->link);
 	free_page((unsigned long)sp->spt);
+	free_page((unsigned long)sp->spt_shadowx);
 	if (!sp->role.direct)
 		free_page((unsigned long)sp->gfns);
 	kmem_cache_free(mmu_page_header_cache, sp);
@@ -1590,9 +1591,11 @@
 
 	sp = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_page_header_cache);
 	sp->spt = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_shadow_page_cache);
+	sp->spt_shadowx = (void*)__get_free_page(GFP_ATOMIC | __GFP_ACCOUNT);
 	if (!direct)
 		sp->gfns = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_gfn_array_cache);
 	set_page_private(virt_to_page(sp->spt), (unsigned long)sp);
+	set_page_private(virt_to_page(sp->spt_shadowx), (unsigned long)sp);
 
 	/*
 	 * active_mmu_pages must be a FIFO list, as kvm_zap_obsolete_pages()
@@ -2100,10 +2103,20 @@
 	__shadow_walk_next(iterator, *iterator->sptep);
 }
 
+/**
+ * @brief 如果不是叶子pte，创建新的中间页表然后进行连接
+ * 
+ * @param vcpu 
+ * @param sptep 当前页表spte
+ * @param sp 被创建的下级页表
+ */
 static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 			     struct kvm_mmu_page *sp)
 {
 	u64 spte;
+	u64 spte_shadowx;
+	struct kvm_mmu_page *sp_shadowx;
+	u64 *sptep_shadowx;
 
 	BUILD_BUG_ON(VMX_EPT_WRITABLE_MASK != PT_WRITABLE_MASK);
 
@@ -2115,6 +2128,22 @@
 
 	if (sp->unsync_children || sp->unsync)
 		mark_unsync(sptep);
+	
+	// ========================================
+	spte_shadowx = make_nonleaf_spte(sp->spt_shadowx, sp_ad_disabled(sp));
+
+	sp_shadowx = sptep_to_sp(sptep);
+
+	sptep_shadowx = sp_shadowx->spt_shadowx;
+
+	if (*sptep_shadowx & SPTE_MMU_PRESENT_MASK) {
+		*sptep_shadowx ^= SPTE_MMU_PRESENT_MASK;
+	}
+
+	mmu_spte_set(sptep_shadowx, spte_shadowx);
+
+	// mmu_page_add_parent_pte(vcpu, sp, sp_shadowx->spt_shadowx); 加入sp的parent_ptes里面，并不存在shadow
+	// ========================================
 }
 
 static void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,
@@ -2544,6 +2573,11 @@
 
 	ret = make_spte(vcpu, pte_access, level, gfn, pfn, *sptep, speculative,
 			can_unsync, host_writable, sp_ad_disabled(sp), &spte);
+	
+	// ==============================
+	make_spte(vcpu, pte_access, level, gfn, pfn, *(sp->spt_shadowx), speculative,
+			can_unsync, host_writable, sp_ad_disabled(sp), &spte);
+	// ==============================
 
 	if (spte & PT_WRITABLE_MASK)
 		kvm_vcpu_mark_page_dirty(vcpu, gfn);
@@ -2872,9 +2906,11 @@
 					      it.level - 1, true, ACC_ALL);
 
 			link_shadow_page(vcpu, it.sptep, sp);
+
 			if (is_tdp && huge_page_disallowed &&
 			    req_level >= it.level)
 				account_huge_nx_page(vcpu->kvm, sp);
+
 		}
 	}
 
@@ -3229,19 +3265,22 @@
 	hpa_t root;
 	unsigned i;
 	int r;
-
+	// pr_info("mmu_alloc_direct_roots\n");
 	write_lock(&vcpu->kvm->mmu_lock);
 	r = make_mmu_pages_available(vcpu);
 	if (r < 0)
 		goto out_unlock;
 
 	if (is_tdp_mmu_enabled(vcpu->kvm)) {
+		// pr_info("mmu_alloc_direct_roots tdp mmu enabled\n");
 		root = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);
 		mmu->root_hpa = root;
 	} else if (shadow_root_level >= PT64_ROOT_4LEVEL) {
+		// pr_info("mmu_alloc_direct_roots PT64_ROOT_4LEVEL enabled\n");
 		root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level, true);
 		mmu->root_hpa = root;
 	} else if (shadow_root_level == PT32E_ROOT_LEVEL) {
+		// pr_info("mmu_alloc_direct_roots PT32_ROOT_LEVEL enabled\n");
 		if (WARN_ON_ONCE(!mmu->pae_root)) {
 			r = -EIO;
 			goto out_unlock;
@@ -3263,12 +3302,48 @@
 	}
 
 	/* root_pgd is ignored for direct MMUs. */
+	mmu->root_hpa_shadowx = mmu->root_hpa;
 	mmu->root_pgd = 0;
 out_unlock:
 	write_unlock(&vcpu->kvm->mmu_lock);
 	return r;
 }
 
+// ====================================================
+// static int mmu_alloc_direct_roots_shadowx(struct kvm_vcpu *vcpu)
+// {
+// 	struct kvm_mmu *mmu = vcpu->arch.mmu_shadowx;
+// 	// u8 shadow_root_level = mmu->shadow_root_level;
+// 	hpa_t root;
+// 	// unsigned i;
+// 	int r;
+
+// 	write_lock(&vcpu->kvm->mmu_lock);
+// 	pr_info("mmu_alloc_direct_roots_shadowx %016lx  %u\n", (unsigned long) vcpu, vcpu->kvm->arch.tdp_mmu_enabled);
+// 	// r = make_mmu_pages_available(vcpu);
+// 	// if (r < 0)
+// 	// 	goto out_unlock;
+// 	r = 0;
+
+// 	if (is_tdp_mmu_enabled(vcpu->kvm)) {
+// 		root = kvm_tdp_mmu_get_vcpu_root_hpa_shadowx(vcpu);
+// 		mmu->root_hpa = root;
+// 		pr_info("mmu_alloc_direct_roots_shadowx root: %016lx\n", (unsigned long) root);
+// 	} 
+// 	pr_info("===================\n===================\n===================\n===================\n===================\n===================\n===================\n===================\n===================\nmmu_alloc_direct_roots_shadowx: %016lx  %016lx\n==========================\n", (unsigned long )vcpu, (unsigned long) mmu->root_hpa);
+
+// 	/* root_pgd is ignored for direct MMUs. */
+// 	mmu->root_pgd = 0;
+// // out_unlock:
+// 	write_unlock(&vcpu->kvm->mmu_lock);
+// 	return r;
+// }
+
+// ====================================================================
+
+
+
+
 static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -4580,6 +4655,7 @@
 	reset_tdp_shadow_zero_bits_mask(vcpu, context);
 }
 
+
 static union kvm_mmu_role
 kvm_calc_shadow_root_page_role_common(struct kvm_vcpu *vcpu, bool base_only)
 {
@@ -4833,16 +4909,31 @@
 
 		vcpu->arch.mmu->root_hpa = INVALID_PAGE;
 
+		// ======================================
+		// vcpu->arch.mmu_shadowx->root_hpa = INVALID_PAGE;
+		// ======================================
+
 		for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
 			vcpu->arch.mmu->prev_roots[i] = KVM_MMU_ROOT_INFO_INVALID;
 	}
 
+	// if (mmu_is_nested(vcpu))
+	// 	init_kvm_nested_mmu(vcpu);
+	// else if (tdp_enabled)
+	// 	init_kvm_tdp_mmu(vcpu);
+	// else
+	// 	init_kvm_softmmu(vcpu);
+	// ===================================
 	if (mmu_is_nested(vcpu))
 		init_kvm_nested_mmu(vcpu);
-	else if (tdp_enabled)
+	else if (tdp_enabled) {
+		// pr_info("kvm_init_mmu tdp_enabled\n");
 		init_kvm_tdp_mmu(vcpu);
+		// init_kvm_tdp_mmu_shadowx(vcpu);
+	}
 	else
 		init_kvm_softmmu(vcpu);
+	// ===================================
 }
 EXPORT_SYMBOL_GPL(kvm_init_mmu);
 
@@ -4888,10 +4979,21 @@
 	r = mmu_alloc_special_roots(vcpu);
 	if (r)
 		goto out;
-	if (vcpu->arch.mmu->direct_map)
+	// if (vcpu->arch.mmu->direct_map)
+	// 	r = mmu_alloc_direct_roots(vcpu);
+	// else
+	// 	r = mmu_alloc_shadow_roots(vcpu);
+
+	// =======================================
+	if (vcpu->arch.mmu->direct_map) {
+		// r = mmu_alloc_direct_roots_shadowx(vcpu);
 		r = mmu_alloc_direct_roots(vcpu);
+		// pr_info("kvm_mmu_load  direct_map r:%u\n", r);
+	}
 	else
 		r = mmu_alloc_shadow_roots(vcpu);
+
+	// ========================================
 	if (r)
 		goto out;
 
@@ -5374,6 +5476,14 @@
 	vcpu->arch.mmu = &vcpu->arch.root_mmu;
 	vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
 
+// ============================================
+	// vcpu->arch.mmu_shadowx = &vcpu->arch.root_mmu_shadowx;
+	// vcpu->arch.walk_mmu_shadowx = &vcpu->arch.root_mmu_shadowx;
+	// ret = __kvm_mmu_create(vcpu, &vcpu->arch.root_mmu_shadowx);
+	// if (ret)
+	// 	return ret;
+// ============================================
+
 	vcpu->arch.nested_mmu.translate_gpa = translate_nested_gpa;
 
 	ret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);
